// Âü∫Á°Ä LLM ÊúçÂä°Á±ª
class BaseLLMService {
    constructor(config) {
        if (new.target === BaseLLMService) {
            throw new Error('BaseLLMService ‰∏çËÉΩÁõ¥Êé•ÂÆû‰æãÂåñ');
        }
        this.config = config;
        this.questionCache = new Map();
    }

    async getAnswer(text) {
        if (this.questionCache.has(text)) {
            console.log('üîÑ ‰ΩøÁî®ÁºìÂ≠òÁöÑÂõûÁ≠î');
            return this.questionCache.get(text);
        }

        try {
            const answer = await this._generateAnswer(text);
            this.questionCache.set(text, answer);
            return answer;
        } catch (error) {
            console.error('‚ùå LLM APIÈîôËØØ:', error);
            throw error;
        }
    }

    _getSystemPrompt() {
        return `‰Ω†ÊòØ‰∏ì‰∏öËß£È¢òÂä©Êâã„ÄÇËØ∑Ê≥®ÊÑèÔºö
1. ËæìÂÖ•ÊñáÊú¨ÂèØËÉΩÂåÖÂê´OCRËØÜÂà´ÈîôËØØ„ÄÅ‰π±Á†ÅÊàñÊó†ÂÖ≥ÊñáÂ≠óÔºåËØ∑Êô∫ËÉΩËØÜÂà´Ê†∏ÂøÉÈóÆÈ¢òÂπ∂ÂøΩÁï•Âπ≤Êâ∞ÂÜÖÂÆπ
2. ÂõûÁ≠îÊ†ºÂºèÂøÖÈ°ªÊòØÔºö„ÄêÁ≠îÊ°à„ÄëÈÄâÈ°π/ÁªìÊûú + ÁÆÄÁü≠Ëß£Èáä
3. ‰∏çË¶ÅÁäπË±´ÔºåÂøÖÈ°ªÁªôÂá∫ÊòéÁ°ÆÁ≠îÊ°à
4. Â¶ÇÊûúÊòØÈÄâÊã©È¢òÔºåÁõ¥Êé•ÁªôÂá∫Ê≠£Á°ÆÈÄâÈ°π
5. Â¶ÇÊûúÊòØÈóÆÁ≠îÈ¢òÔºåÁªôÂá∫ÁÆÄÊ¥ÅÊòéÁ°ÆÁöÑÁ≠îÊ°à
6. ‰∏çË¶ÅËØ¥'ÊàëËÆ§‰∏∫'Êàñ'ÂèØËÉΩ'Á≠âÊ®°Á≥äË°®Ëææ
7. Ëã±ÊñáÈóÆÈ¢òÁî®Ëã±ÊñáÂõûÁ≠îÔºåÊ†ºÂºè‰∏∫Ôºö„ÄêAnswer„Äëoption/result + brief explanation`;
    }

    async _generateAnswer(text) {
        throw new Error('_generateAnswer ÊñπÊ≥ïÂøÖÈ°ªÁî±Â≠êÁ±ªÂÆûÁé∞');
    }

    async _makeRequest(endpoint, requestBody, apiKey) {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 20000);

        try {
            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Accept': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify(requestBody),
                signal: controller.signal
            });

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`APIËØ∑Ê±ÇÂ§±Ë¥•: ${response.status}, ${errorText}`);
            }

            return await response.json();
        } finally {
            clearTimeout(timeoutId);
        }
    }
}

// SiliconFlow ÂÆûÁé∞
class SiliconFlowService extends BaseLLMService {
    async _generateAnswer(text) {
        const { SILICONFLOW_API_KEY, SILICONFLOW_API_ENDPOINT, SILICONFLOW_MODEL } = this.config;
        
        const requestBody = {
            model: SILICONFLOW_MODEL,
            messages: [
                { role: "system", content: this._getSystemPrompt() },
                { role: "user", content: text }
            ]
        };

        const response = await this._makeRequest(
            SILICONFLOW_API_ENDPOINT,
            requestBody,
            SILICONFLOW_API_KEY
        );

        return response.choices[0].message.content;
    }
}

// DeepSeek ÂÆûÁé∞
class DeepSeekService extends BaseLLMService {
    async _generateAnswer(text) {
        const { DEEPSEEK_API_KEY, DEEPSEEK_API_ENDPOINT } = this.config;
        
        const requestBody = {
            model: "deepseek-chat",
            messages: [
                { role: "system", content: this._getSystemPrompt() },
                { role: "user", content: text }
            ]
        };

        const response = await this._makeRequest(
            DEEPSEEK_API_ENDPOINT,
            requestBody,
            DEEPSEEK_API_KEY
        );

        return response.choices[0].message.content;
    }
}

// LLM ÊúçÂä°Â∑•ÂéÇ
class LLMServiceFactory {
    static create(config) {
        const { LLM_MODEL } = config;
        
        switch (LLM_MODEL.toLowerCase()) {
            case 'siliconflow':
                return new SiliconFlowService(config);
            case 'deepseek':
                return new DeepSeekService(config);
            default:
                throw new Error(`‰∏çÊîØÊåÅÁöÑ LLM Ê®°Âûã: ${LLM_MODEL}`);
        }
    }
}

export { LLMServiceFactory }; 